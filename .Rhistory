head(flag_colors)
lapply(flag_colors,sum)
sapply(flag_colors,sum)
sapply(flag_colors,mean)
flag_shapes <- flags[, 19:23]
lapply(flag_shapes,range)
shape_mat<-sapply(flag_shapes,range)
shape_mat
class(shape_mat)
unique(c(3, 4, 5, 5, 5, 6, 6))
unique_vals<-lapply(flags,unique)
unique_vals
sapply(unique_vals,length)
sapply(flags,unique)
lapply(unique_vals, function(elem) elem[2])
?str
sapply(flags, unique)
vapply(flags, unique, numeric(1))
ok()
sapply(flags, class)
vapply(flags, class, character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean)
tapply(flags$population, flags$red, summary)
tapply(flags$population, flags$landmass, summary)
?retunr
?return
source('E:/Data Science/complete.r')
source('E:/Data Science/makevector.r')
makeVector(\)
makeVector()
x$getmean()
c[1,2,3,4,5,6]
c(1,2,3,4,5,6)
a<-c(1,2,3,4,5,6)
makevector(a)
makeVector(a)
x$getmean
source('E:/Data Science/makevector.r')
source('E:/Data Science/makevector.r')
cachemean()
cachemean(a)
set()
m
x
makeVector
x
makeVector()
x
m
getmean
m
x
x<-null
x<-NULL
makeVector()
x
m
class(a)
makeVector(a)
x
set
makeVector$set
makeVector$set(a)
makeVector()
x<-makeVector()
x
cachemean()
cachemean(x)
cachemean(a)
source('E:/Data Science/makevector.r')
myVector<-makeVector(1:15)
myVector
x
x
x<-numeric()
myVector<-makeVector(1:15)
aVector <- makeVector(1:10)
aVector$get()
aVector
aVector <- makeVector(1:10)
source('E:/Data Science/makevector.r')
aVector <- makeVector(1:10)
class(aVector)
aVector[[1]]
aVector
aVector$get()
aVector$getmean()
aVector$set(30:50)
aVector$get()
cachemean(aVector)
aVector$getmean()
str(aVector)
aVector$x
aVector$m
?numeric
?matrix
source('E:/Data Science/makevector.r')
source('E:/Data Science/makeCacheMatrix.r')
source('E:/Data Science/makeCacheMatrix.r')
matrix()
matrix(c(1,2,3,4,5,6), nrow=3, ncol=3)
aMatrix<- makeCacheMatrix(matrix(c(1,2,3,4,5,6), nrow=3, ncol=3))
aMatrix$get()
aMatrix$getsolve()
source('E:/Data Science/makeCacheMatrix.r')
cacheSolve(aMatrix)
aMatrix$set(matrix(c(4,2,7,6), nrow=2, ncol=2))
cacheSolve(aMatrix)
aMatrix$getsolve()
aMatrix$m
m
m1 <- matrix(c(1/2, -1/4, -1, 3/4), nrow = 2, ncol = 2)
myMatrix_object <- makeCacheMatrix(m1)
cacheSolve(myMatrix_object)
cacheSolve(myMatrix_object)
n2 <- matrix(c(5/8, -1/8, -7/8, 3/8), nrow = 2, ncol = 2)
myMatrix_object$set(n2)
cacheSolve(myMatrix_object)
cacheSolve(myMatrix_object)
library(datasets)
data(iris)
?iris
library(datasets)
data(iris)
?iris
mean(iris)
head(iris)
tapply(iris$Sepal.Length,iris$Species,mean)
rowMeans(iris[, 1:4])
class(rowMeans(iris[, 1:4]))
?apply
apply(iris, 2, mean)
apply(iris[, 1:4], 1, mean)
apply(iris[, 1:4], 2, mean)
class(apply(iris[, 1:4], 2, mean))
apply(iris, 1, mean)
colMeans(iris)
library(datasets)
data(mtcars)
?mtcars
tapply(mtcars$cyl, mtcars$mpg, mean)
head(mtcars)
tapply(mtcars$mpg, mtcars$cyl, mean)
class(mtcars)
apply(mtcars, 2, mean)
?apply
?with
with(mtcars, tapply(mpg, cyl, mean))
split(mtcars, mtcars$cyl)
sapply(split(mtcars$mpg, mtcars$cyl), mean)
sapply(mtcars, cyl, mean)
tapply(mtcars$cyl, mtcars$mpg, mean)
lapply(mtcars, mean)
mean(mtcars$mpg, mtcars$cyl)
tapply(mtcars$hp, mtcars$cyl, mean)
a<-tapply(mtcars$hp, mtcars$cyl, mean)
class(a)
a[1]-a[2]
a[1]-a[8]
a[1]-a[2]
debug(ls)
ls
?debug
fileUrlXML<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
restaurantData<-xmlTreeParse(fileUrlXML, useInternalNodes = TRUE)
library(XML)
library(XML)
install.packages("XML")
library(XML)
fileUrlXML <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
restaurantData <- xmlTreeParse(fileUrlXML,useInternalNodes = TRUE)
install.packages("curl")
curlVersion()$features
library(curl)
curlVersion()$features
curl_version()$features
curl_version()$protocol
restaurantData <- getURL("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", curl=curl)
install.packages("RCurl")
restaurantData <- getURL("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", curl=curl)
require(RCurl)
restaurantData <- getURL("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", curl=curl)
curl<-getCurlHandle()
getURL("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", ssl.verifyPeer=FALSE)
restaurantData <- getURL("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", ssl.verifyPeer=FALSE)
doc<-xmlTreeParse(restaurantData,useInternalNodes = TRUE)
rootNode<-xmlRoot(doc)
xmlName(rootNode)
xpathSApply(rootNode,"//zipcode",xmlValue)
zipcodes<-xpathSApply(rootNode,"//zipcode",xmlValue)
table(zipcodes)
?require
str(restaurantData)
class(restaurantData)
is.character(restaurantData)
fileURL<- https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv
fileURL<- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "ACS.csv")
?fread
??Fread
install.packages("data.table")
?fread
library(data.table)
?fread
fread("ACS.csv")
DT<-fread("ACS.csv")
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
system.time({rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]})
DT
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time({})
system.time({rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]})
system.time({rowMeans(DT)[DT$SEX==1] rowMeans(DT)[DT$SEX==2]})
system.time({rowMeans(DT)[DT$SEX==1] rowMeans(DT)[DT$SEX==2]})
system.time({
rowMeans(DT)[DT$SEX==1]
rowMeans(DT)[DT$SEX==2]
})
?rowMeans
DT
rowMeans(DT)[DT$SEX==1]
DT[,mean(pwgtp15),by=SEX]
install.packages("httr")
install.packages("httpuv")
myapp <- oauth_app("github",
key = "3ba6c38e4b1475334b22",
secret = "0d8b301550e2015747065b6bb21aaf34c5e0e30d")
library(httr)
library(httpuv
)
library(httpuv)
library(httpuv)
install.packages("httpuv")
myapp <- oauth_app("github",
key = "3ba6c38e4b1475334b22",
secret = "0d8b301550e2015747065b6bb21aaf34c5e0e30d")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
local({pkg <- select.list(sort(.packages(all.available = TRUE)),graphics=TRUE)
if(nchar(pkg)) library(pkg, character.only=TRUE)})
utils:::menuInstallPkgs()
q()
library(httpuv)
install.packages(c("Rcpp", "httpuv", "shiny"))
library(httpuv)
require(httpuv)
local({pkg <- select.list(sort(.packages(all.available = TRUE)),graphics=TRUE)
if(nchar(pkg)) library(pkg, character.only=TRUE)})
update.packages(ask='graphics',checkBuilt=TRUE)
local({pkg <- select.list(sort(.packages(all.available = TRUE)),graphics=TRUE)
if(nchar(pkg)) library(pkg, character.only=TRUE)})
utils:::menuInstallPkgs()
library(httpuv)
remove.packages(httpuv
)
remove.packages(httpuv)
remove.packages()
remove.packages(httr)
remove.packages(htpuv)
remove.packages(httpuv)
library(httpuv)
utils:::menuInstallPkgs()
q()
install.packages("httpuv")
library(httpuv)
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github",key = "3ba6c38e4b1475334b22",secret = "8e107541ae1791259e9987d544ca568633da2eb0d8b301550e2015747065b6bb21aaf34c5e0e30df")
myapp <- oauth_app("github",key = "3ba6c38e4b1475334b22",secret = "0d8b301550e2015747065b6bb21aaf34c5e0e30d")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
libtary(httr)
library(httr)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(httr)
myapp <- oauth_app("github",key = "3ba6c38e4b1475334b22",secret = "0d8b301550e2015747065b6bb21aaf34c5e0e30d")
oauth_endpoints("github")
myapp <- oauth_app("github",key = "3ba6c38e4b1475334b22",secret = "0d8b301550e2015747065b6bb21aaf34c5e0e30d")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/rate_limit", gtoken)
stop_for_status(req)
content(req)
gtoken
req <- GET("https://api.github.com/rate_limit", gtoken)https://api.github.com/users/jtleek/repos
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
req
stop_for_status(req)
content(req)
?content
str(req)
class(req)
head(Req)
head(req)
json1=content(req)
json2=jsonlite::fromJSON(toJSON(json1))
library(jsonlite)
json2=jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
class(json2)
table(json2)
str(json2)
dim(json2)
class(json2)
head(json2)
json2$name
dim(json2)
grepl("datasharing",json2$name)
isdatasharing<-grepl("datasharing",json2$name)
class(isdatasharing)
which(isdatasharing)
json2[[12]]$created_at
json2[[12]]
json2[[1]]
names(json2)
names(json2)
json2[json2$name=="datasharing"]
json2[json2$name=="datasharing",]
json2[json2$name=="datasharing",json2$created_at]
subset(json2,name=="datasharing",select="created_at")
q()
q()
library(swirl)
swirl()
?swirl
install_course("Manipulating Data with dplyr")
install.packages(swirl)
install.packages("swirl")
install_course("Manipulating Data with dplyr")
library
(swirl)
library(swirl)
install_course("Manipulating Data with dplyr")
install_from_swirl("Getting and Cleaning Data")
swirl()
install.packages("data.table")
q()
source('E:/Data Science/Cleaning Data/run_analysis.R')
run_analysis()
DatasetURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
localfile<-"ss06hid.csv"
download.file(DatasetURL,destfile = localfile,mode = "wb")
read.csv(file = localfile)
ss06hid<-read.csv(file = localfile)
View(ss06hid)
colnames(ss06hid)
?strsplit
colnames<-colnames(ss06hid)
strsplit(colnames,"wgtp")
strsplit(colnames[123],"wgtp")
DatasetURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
localfile<-FGDP.csv
localfile<-"FGDP.csv"
download.file(DatasetURL,destfile = localfile,mode = "wb")
read.csv(file = localfile)
GDP<-read.csv(file = localfile)
View(GDP)
?SUB
?sub
GDPclean<-sub("\\,","",GDP$X.3)
GDPclean
GDPclean<-Gsub("\\,","",GDP$X.3)
GDPclean<-gsub("\\,","",GDP$X.3)
GDPclean
GDPclean[5:194]
?mean
mean(GDPclean[5:194])
class(GDPclean[5:194])
as.numeric(GDPclean[5:194])
mean(as.numeric(GDPclean[5:194]))
countryNames<-GDPclean$X[5:194]
GDPclean\
GDPclean
countryNames<-GDP$X[5:194]
grep("^United",countryNames)
countryNames<-GDP$X.2[5:194]
grep("^United",countryNames)
DatasetURL
DatasetURL2<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv""
DatasetURL2<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
localfile2<-"edstats.csv"
download.file(DatasetURL2,destfile2 = localfile,mode = "wb")
download.file(DatasetURL2,destfile = localfile2,mode = "wb")
read.csv(localfile2)
edstats<-read.csv(localfile2)
View(edstats)
GDPclean<-GDP[5:194,]
View(GDPclean)
mergedData<-merge(GDPclean,edstats,by.x="X",by.y="CountryCode",all=TRUE)
View(mergedData)
grep("*Fiscal*June*",mergedData$Special.Notes)
grep("^Fiscal*June*",mergedData$Special.Notes)
grep("^Fiscal*",mergedData$Special.Notes)
grep("^Fiscal*une",mergedData$Special.Notes)
grep("^Fiscal*une*",mergedData$Special.Notes)
grep("^Fiscal.*une.*",mergedData$Special.Notes)
length(grep("^Fiscal.*une.*",mergedData$Special.Notes))
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
install.packages("quantmod")
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
sampleTimes
amzn
?getSymbols
?index
class(sampleTimes)
weekdays(sampleTimes)
weekdays<-weekdays(sampleTimes)
length(grep("Monday",weekdays))
years(sampleTimes)
sampleTimes
install.packages("lubridate")
library(lubridate)
year(sampleTimes)
year<-year(sampleTimes)
class(year)
length(grep("2012",weekdays))
year[year=2012]
year[year=2012,]
year[year==2012,]
year[year==2012]
length(year[year==2012])
which(year[year==2012])
year==2012
which(year==2012)
which(!year==2012)
cleanyear<-year(-which(!year==2012))
rows<-which(!year==2012)
rows
!year==2012
years(!year==2012)
years
year
year(!year==2012)
year
class(year)
!year==2012
year==2012
!year==2012
year[!year==2012]
year[year==2012]
length(year[year==2012])
library(swirl)
install_from_swirl("Exploratory Data Analysis")
swirl()
head(pollution)
dim(pollution)
summary(pollution$pm25)
quantile(ppm)
boxplot(ppm,col =  "blue")
abline(h=12)
hist(popm,col="green")
hist(ppm,col="green")
rug(ppm)
low
high
hist(ppm,col="green",breaks=100)
rug(ppm)
hist(ppm,col="green")
abline(v=12,lwd=2)
abline(v=median(ppm),col="magenta",lwd=4)
names(pollution)
reg<-table(pollution$region)
reg
barplot(reg,col="wheat",main="Number of Counties in Each Region")
boxplot(y~x,data=pollution,col="red")
boxplot(pm25~region,data=pollution,col="red")
par(mfrow=c(2,1),mar=c(4,4,2,1))
east<-subset(pollution,region=="east")
head(east)
hist(east$pm25,col="green")
hist(subset(pollution,region=="west"),col="green")
hist(subset(pollution$pm25,region=="west"),col="green")
west<-subset(pollution,region=="wast")
hist(subset(pollution,region=="west")$pm25,col="green")
plot(x = lati\
plot(x = lati
)
plot(pollution)
Type with(pollution, plot(latitude, pm25)) at the command prompt.
Type with(pollution, plot(latitude, pm25))
with(pollution, plot(latitude, pm25))
abline(lwd=2,lty=2)
abline(h=12,lwd=2,lty=2)
plot(pollution$latitude,ppm,col=pollution$region)
abline(h=12,lwd=2,lty=2)
par(mfrow = c(1, 2), mar = c(5, 4, 2, 1))
west<-subset(pollution,region="west")
west<-subset(pollution,region=="west")
plot(west$latitude,west$pm25,main="west")
plot(west$latitude,west$pm25,main="west")
plot(west$latitude, west$pm25, main = "West")
plot(east$latitude, east$pm25, main = "east")
plot(east$latitude, east$pm25, main = "East")
setwd("~/GitHub/RepData_PeerAssessment1")
knit2html()
install.packages("knitr")
knit2html()
library(knitr)
knit2html()
?knit2html
knit2html("PA1_template.Rmd")
